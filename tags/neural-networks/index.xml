<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks on Maciej Odziemczyk</title>
    <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/tags/neural-networks/</link>
    <description>Recent content in Neural Networks on Maciej Odziemczyk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 May 2021 11:25:05 -0400</lastBuildDate><atom:link href="https://maciejodziemczyk.github.io/DataSciencePortfolio/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>News headers classification - NLP</title>
      <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/news_headers_classification/</link>
      <pubDate>Wed, 26 May 2021 11:25:05 -0400</pubDate>
      
      <guid>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/news_headers_classification/</guid>
      <description>This project is about news headers classification (text mining field), but the project core was preprocessing and feature engineering, you can expect:
 target variable analysis a lot of data preprocessing (cleaning with regex, stoplist building, lemmatization, all verified with word clouds) a lot of feature engineering  titles length measures (before and after preprocessing, mean number of words, normalized) sentiment estimated with VADER (normalized) named entity recognition with SpaCy (groups of tags, most frequent tags) n-grams (most frequent unigrams and bigrams)   GloVe word embeddings (100-dimensional vectors) model validation and architekture choosing final two input neural network architecture:  first branch based on pretrained GloVe embeddings, 1D Convolutions and 1D Max pooling second branch based on engineered features (65 in total, sparse) and dense layer concatenation to main branch with two more dense layers with regularization and dropout softmax classifier with 4 outputs   model evaluation and amazing test set results (at least 90% accuracy for each class, test set confusion matrix below) Jupyter Notebook in English with ton of python code :)   GitHub repository</description>
    </item>
    
    <item>
      <title>Cross-sectional tabular data modelling with CNNs</title>
      <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/cross-sectional_tabular_data_modelling_with_cnns/</link>
      <pubDate>Sun, 02 May 2021 11:25:05 -0400</pubDate>
      
      <guid>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/cross-sectional_tabular_data_modelling_with_cnns/</guid>
      <description>This project is a very big part of my Master Thesis at WNE UW, you can expect:
 data preprocessing (missing detection and imputation with earlier developed RF based algorithm, data manipulation, image to vector conversion using Monte Carlo simulation) data analysis and transforms (balance check, distributions analysis (box plots and histograms), outliers reduction using quantile clipping, Yeo-Johnson power transform, normalization, special normalization variant for CNNs) n-Fold Cross Validation study on training set and testing on test set several algorithms (Logits, Random Forests, XGBoosts, Feedforward Neural Networks, Convolutional Neural Networks) hyperparameters optimization for Logits (regularization only), RFs and XGBs Networks building (experiments based on results and learning curves) optimized Inception modules for CNNs automatic feature generation method for enlarging CNNs inputs, based on composing sampled features and arithmetic operations from created discrete probability distributions many times CV results comparison (Wilcoxon testing) models training on the entire training set (specifications chosen in CV) models testing and comparison study on 5 datasets - companies bankruptcy prediction for 1-5 years forecast horizons (in progress) Python notebooks in English (a ton of Python code)   GitHub repository</description>
    </item>
    
    <item>
      <title>IMDB movie reviews - sentiment analysis</title>
      <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/imdb_movie_reviews-sentiment_analysis/</link>
      <pubDate>Sun, 07 Feb 2021 11:25:05 -0400</pubDate>
      
      <guid>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/imdb_movie_reviews-sentiment_analysis/</guid>
      <description>This project is about sentiment analysis (text mining field), you can expect:
 data preparation (nulls and balance check, regex cleaning, label decoding, review length constraint, tokenization, vocabulary building) working with pretrained word embeddings (GloVe) data analysis (embedding coverage, detailed cleaning with regex, stop words, word clouds and count plots) modelling (CNNs, RNNs, VADER) with Cross Validation and error analysis (confusion matrix, misclassification examples analysis) experiments (review length and GloVe dimension sensitivity) 86% test set accuracy Jupyter Notebook report in English (a lot of Python code)   GitHub repository</description>
    </item>
    
    <item>
      <title>Can PCA extract important informations from non-significant features? Neural Network case</title>
      <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/can_pca_extract_important_informations_from_non-sigificant_features_neural_network_case/</link>
      <pubDate>Sat, 09 Jan 2021 11:25:05 -0400</pubDate>
      
      <guid>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/can_pca_extract_important_informations_from_non-sigificant_features_neural_network_case/</guid>
      <description>This project is about boosting Neural Networks with PCA (other ML algorithms as benchmarks), you can expect:
 data preparation (renaming labels, balance check, standarization) Random Forest based data imputation algorithm development n-Fold Cross Validation study Machine Learning algorithms (Random Forest, XGBoost with hiperparameters optimization) 6 feature selection methods to spot non-significant features (RF importance, Mutual Information, Spearman correlation between features and with target, General to Specific econometrics procedure, Lasso logistic regression) Neural Networks development (architecture, optimizers, activations, regularization, dropout, batch norm, hyperparameters) Principal Component Analysis of the dataset PCA integration with Nets in CV hypothesis verification using the Wilcoxon test for equality of medians models comparison Python notebook in English (a ton of Python code) quality paper-style report in English project presentation in English   GitHub repository</description>
    </item>
    
    <item>
      <title>Churn Modelling</title>
      <link>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/churn_modelling/</link>
      <pubDate>Tue, 09 Jun 2020 11:14:48 -0400</pubDate>
      
      <guid>https://maciejodziemczyk.github.io/DataSciencePortfolio/post/churn_modelling/</guid>
      <description>This project is about churn modelling (binary classification task) with Machine Learning and Neural Networks in Python, you can expect:
 statistical analysis (t-student and Jarque-Bera tests, Correlations, Mutual Information) feature selection (feature importances, sensitivity, statistical methods) feature engineering (monotonic transforms, binarization, label encoding, interactions) lots of ML algorithms (Logits, Random Forests, XGBoosts, SVMs, kNNs, Neural Nets, ensembling) n-Folds Cross Validation study bootstrap simulation several notebooks in Polish (lots of Python code)   GitHub repository</description>
    </item>
    
  </channel>
</rss>
